# pr_pilot/git_providers/github.py

from github import Github, GithubException, UnknownObjectException, PullRequest, Repository
from .base import GitProvider
import config
import yaml
import os
from utils import graph_parser
from .exceptions import PullRequestNotFound
from rag import retriever
from utils import code_parser
from utils.language_registry import registry as lang_registry
from utils.callgraph_builder import load_callgraph_json
import numpy as np
import chromadb

# ç¡®ä¿å¯¼å…¥äº†æ‰€æœ‰éœ€è¦çš„æ•°æ®ç»“æ„å’Œæ–°åˆ†æå™¨
from analysis.base import PRContext
from analysis.history_analyzer import HistoryAnalyzer
from analysis.clone_analyzer import CloneAnalyzer

from typing import Optional

class GitHubProvider(GitProvider):
    def __init__(self, token: str = config.GITHUB_TOKEN):
        if not token:
            raise ValueError("GitHub token is not configured.")
        self.client = Github(token)
        self.db_client = chromadb.PersistentClient(path=config.CHROMA_DB_PATH)
        print("Successfully connected to GitHub API.")

    def get_pr_metadata(self, repo_name: str, pr_number: int) -> tuple[Repository.Repository, PullRequest.PullRequest]:
        print(f"Checking metadata for {repo_name} PR #{pr_number}...")
        try:
            repo = self.client.get_repo(repo_name)
            pr = repo.get_pull(pr_number)
            if pr.state != 'open':
                raise PullRequestNotFound(f"PR #{pr_number} state is '{pr.state}', not 'open'.")
            print(f"PR is open: '{pr.title}'")
            return repo, pr
        except UnknownObjectException:
            raise PullRequestNotFound(f"PR #{pr_number} in repo {repo_name} does not exist.")
        except GithubException as e:
            print(f"GitHub API error during metadata check: {e}")
            raise

    # ã€ã€ã€ V3.0 æœ€ç»ˆç‰ˆ build_context_from_pr ã€‘ã€‘ã€‘
    def build_context_from_pr(self, repo: Repository.Repository, pr: PullRequest.PullRequest, use_rag: bool = True, retrieval_mode: str = 'diff', analysis_mode: str = 'two-stage', top_k: int = 5) -> PRContext:
        print("Building structured context for the PR (V3.0)...")

        # 1. åˆå§‹åŒ–å®Œæ•´çš„ PRContext ç»“æ„
        pr_context: PRContext = {
            "main_context": "",
            "lean_main_context": "",
            "repo_config": {},
            "related_snippets": [],
            "dependency_chains": [],
            "historical_warnings": [],
            "clone_warnings": [],
        }

        # 2. å¡«å……åŸºæœ¬ä¿¡æ¯
        pr_context["repo_config"] = self._load_project_configuration(repo)
        call_graph = self._load_call_graph(repo)
        
        # 3. åˆ†ææ–‡ä»¶å˜æ›´ (è°ƒç”¨ V3.0 ç‰ˆçš„å‡½æ•°)
        file_changes_analysis = self._analyze_file_changes(pr, repo)
        
        pr_header = (
            f"Pull Request #{pr.number}: {pr.title}\n"
            f"Author: {pr.user.login}\n---\nPR Description:\n{pr.body or 'No description provided.'}\n---\n"
        )
        
        # 4. æ­£ç¡®åœ°ä¸ºä¸¤ä¸ªä¸Šä¸‹æ–‡èµ‹å€¼
        pr_context["main_context"] = pr_header + file_changes_analysis["context_for_prompt"]
        pr_context["lean_main_context"] = pr_header + file_changes_analysis["lean_context_for_prompt"]

        # 5. è¿è¡Œæ‰€æœ‰å¢å¼ºåˆ†ææ¨¡å—
        if use_rag and (file_changes_analysis["diff_query_text"] or file_changes_analysis["changed_functions_code"]):
            pr_context["related_snippets"], pr_context["dependency_chains"] = self._augment_context_with_rag(
                repo, call_graph, file_changes_analysis, retrieval_mode, top_k
        
            )

        repo_short_name = repo.full_name.split('/')[-1]
        repo_name_cleaned = repo_short_name.replace('.', '_').replace('-', '_')

        history_analyzer = HistoryAnalyzer(repo_name_cleaned)
        changed_files_for_history = file_changes_analysis["changed_files"]
        pr_context["historical_warnings"] = history_analyzer.analyze(changed_files_for_history)

        clone_analyzer = CloneAnalyzer(repo_name_cleaned)
        pr_context["clone_warnings"] = clone_analyzer.analyze(file_changes_analysis["changed_function_locations"])

        print("âœ… Successfully built comprehensive structured PR context.")
        return pr_context


    def _load_project_configuration(self, repo: Repository.Repository) -> dict:
        """ã€ä¸“ä¸šæ¨¡å— 1ã€‘åŠ è½½ .pr-pilot.yml é…ç½®æ–‡ä»¶ã€‚"""
        # (æ­¤å‡½æ•°ä¸ä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒ)
        try:
            config_content = repo.get_contents(".pr-pilot.yml").decoded_content.decode("utf-8")
            print("Successfully loaded .pr-pilot.yml config from the repository.")
            return yaml.safe_load(config_content)
        except UnknownObjectException:
            print("No .pr-pilot.yml config file found in the repository. Using default settings.")
            return {}
        except Exception as e:
            print(f"Error loading .pr-pilot.yml: {e}")
            return {}

    def _load_call_graph(self, repo: Repository.Repository) -> dict:
        """ã€ä¸“ä¸šæ¨¡å— 2ã€‘åŠ è½½å¹¶åˆå¹¶é¡¹ç›®çš„å…¨å±€è°ƒç”¨å›¾ï¼ˆpyan .dot + tree-sitter .jsonï¼‰ã€‚"""
        repo_short_name = repo.name
        repo_name_cleaned = repo_short_name.replace('.', '_').replace('-', '_')
        call_graph = {}

        # Load pyan .dot call graph (Python)
        dot_file_path = os.path.join(config.CALL_GRAPH_DIR, f"{repo_name_cleaned}_call_graph.dot")
        if os.path.exists(dot_file_path):
            print(f"Loading pyan call graph from: {dot_file_path}")
            parsed_data = graph_parser.parse_dot_file(dot_file_path)
            if parsed_data:
                print("âœ… Pyan call graph loaded successfully.")
                call_graph.update(parsed_data)
            else:
                print("âš ï¸ Warning: Pyan call graph file exists but failed to parse.")

        # Load tree-sitter .json call graphs (C/C++/Java/Go/JS/TS)
        for ts_lang in ("c", "cpp", "java", "go", "javascript", "typescript"):
            json_file_path = os.path.join(config.CALL_GRAPH_DIR, f"{repo_name_cleaned}_ts_{ts_lang}_call_graph.json")
            if os.path.exists(json_file_path):
                print(f"Loading tree-sitter {ts_lang} call graph from: {json_file_path}")
                ts_graph = load_callgraph_json(json_file_path)
                if ts_graph:
                    # Merge: for overlapping keys, combine edge lists
                    for node, edges in ts_graph.items():
                        if node in call_graph:
                            existing = set(call_graph[node])
                            existing.update(edges)
                            call_graph[node] = list(existing)
                        else:
                            call_graph[node] = edges
                    print(f"âœ… Tree-sitter {ts_lang} call graph merged ({len(ts_graph)} nodes).")

        if not call_graph:
            print("âš ï¸ Warning: No call graph files found. Skipping dependency chain analysis.")
        return call_graph

    def _analyze_file_changes(self, pr: PullRequest.PullRequest, repo: Repository.Repository) -> dict:
        analysis_result = {
            "changed_files": [],
            "changed_function_locations": {},
            "changed_function_nodes": set(),
            "changed_functions_code": [],
            "diff_query_text": "",
            "context_for_prompt": "",           # å®Œæ•´ç‰ˆä¸Šä¸‹æ–‡
            "lean_context_for_prompt": "File Changes (Diffs Only):\n" # ç˜¦èº«ç‰ˆä¸Šä¸‹æ–‡
        }
        
        all_diffs_for_rag_query = []
        full_context_list = ["File Changes:\n"]
        lean_context_list = []

        # 2. éå† PR ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        files = pr.get_files()
        for file in files:
            # åªå¤„ç†è¢«ä¿®æ”¹æˆ–æ–°å¢çš„å—æ”¯æŒè¯­è¨€æ–‡ä»¶
            if file.status not in ['modified', 'added'] or not lang_registry.is_supported(file.filename):
                continue
            
            analysis_result["changed_files"].append(file.filename)
            
            try:
                # è·å–æ–‡ä»¶çš„å®Œæ•´å†…å®¹ï¼ˆç”¨äºå®Œæ•´ç‰ˆä¸Šä¸‹æ–‡ï¼‰
                full_content = repo.get_contents(file.filename, ref=pr.head.sha).decoded_content.decode("utf-8")
                # è§£ææ–‡ä»¶ï¼Œè·å–å‡½æ•°/ç±»çš„å®šä¹‰å’Œä½ç½®
                file_definitions = code_parser.parse_file_content(full_content, file.filename)
            except Exception as e:
                print(f"Could not get or parse content for file {file.filename}: {e}")
                continue

            # 3. è§£æ Diffï¼Œæ‰¾å‡ºè¢«ä¿®æ”¹çš„å…·ä½“è¡Œå·
            changed_lines = set()
            patch_lines = (file.patch or "").split('\n')
            line_num_in_file = 0
            for line in patch_lines:
                if line.startswith('@@'):
                    try:
                        line_num_in_file = int(line.split(' ')[2].split(',')[0].strip('+'))
                    except (ValueError, IndexError): 
                        line_num_in_file = 0 # å¦‚æœè§£æå¤±è´¥ï¼Œä»å¤´å¼€å§‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨çš„å›é€€
                        continue
                elif line.startswith('+') and not line.startswith('+++'):
                    changed_lines.add(line_num_in_file)
                    line_num_in_file += 1
                elif not line.startswith('-'):
                    line_num_in_file += 1
            
            # 4. è¯†åˆ«è¢«ä¿®æ”¹çš„å…·ä½“å‡½æ•°/ç±»
            module_path = lang_registry.strip_extension(file.filename).replace('/', '.').replace('\\', '.')
            for name, code, start, end in file_definitions:
                # å¦‚æœå‡½æ•°/ç±»çš„è¡Œå·èŒƒå›´ä¸è¢«ä¿®æ”¹çš„è¡Œå·æœ‰äº¤é›†
                if not changed_lines.isdisjoint(range(start, end + 1)):
                    # ç±»æ–¹æ³•åæ ¼å¼ "ClassName.method" â†’ "module__ClassName__method"
                    sanitized_name = name.replace('.', '__')
                    full_name = f"{module_path.replace('.', '__')}__{sanitized_name}"
                    analysis_result["changed_function_nodes"].add(full_name)
                    analysis_result["changed_functions_code"].append(code)
                    analysis_result["changed_function_locations"][file.filename] = (start, end)
            
            diff_patch = file.patch or "No diff available."
            all_diffs_for_rag_query.append(diff_patch)

            # 5. åŒæ—¶ä¸ºä¸¤ç§ä¸Šä¸‹æ–‡åˆ—è¡¨æ·»åŠ å†…å®¹
            # --- ä¸ºå®Œæ•´ç‰ˆä¸Šä¸‹æ–‡æ‹¼æ¥ ---
            full_context_list.append(f"--- Modified File: {file.filename} ---")
            full_context_list.append("\n<< FILE CONTENT >>\n")
            full_context_list.append(full_content)
            full_context_list.append("\n<< DIFF >>\n")
            full_context_list.append(diff_patch)
            full_context_list.append("\n<< END DIFF >>\n")

            # --- ä¸ºç˜¦èº«ç‰ˆä¸Šä¸‹æ–‡æ‹¼æ¥ ---
            lean_context_list.append(f"--- Diff for {file.filename} ---")
            lean_context_list.append(diff_patch)
            lean_context_list.append("\n")

        # 6. å°†åˆ—è¡¨æ‹¼æ¥æˆæœ€ç»ˆçš„å­—ç¬¦ä¸²å¹¶å­˜å…¥è¿”å›å­—å…¸
        analysis_result["diff_query_text"] = "\n".join(all_diffs_for_rag_query)
        analysis_result["context_for_prompt"] = "\n".join(full_context_list)
        analysis_result["lean_context_for_prompt"] += "\n".join(lean_context_list)
        
        return analysis_result

    def _augment_context_with_rag(self, repo: Repository.Repository, call_graph: dict, file_changes_analysis: dict, retrieval_mode: str, top_k: int) -> tuple[list[str], list[str]] :
        """
        ã€A/B/C æ ¸å¿ƒæˆ˜åœºã€‘æ‰§è¡Œ RAG å’Œä¾èµ–é“¾å¢å¼ºï¼Œæ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒæŸ¥è¯¢ç­–ç•¥ã€‚
        """
        print(f"\nAugmenting context using '{retrieval_mode}' retrieval mode...")
        
        query_text = ""
        query_embedding = None
        
        collection_name = repo.name.replace('.', '_').replace('-', '_')

        # ================== A/B/C æµ‹è¯•é€»è¾‘åˆ†æ”¯ START ==================
        if retrieval_mode == 'precise':
            if file_changes_analysis["changed_functions_code"]:
                print("Strategy: Using new code from changed functions.")
                query_text = "\n".join(file_changes_analysis["changed_functions_code"])
            else:
                print("Warning: 'precise' mode selected but no changed function code found. Falling back to 'diff'.")
                query_text = file_changes_analysis['diff_query_text']
        
        elif retrieval_mode == 'fast':
            changed_nodes = file_changes_analysis['changed_function_nodes']
            if changed_nodes:
                print(f"Strategy: Using pre-computed vectors for {len(changed_nodes)} changed functions.")
                try:
                    collection = self.db_client.get_collection(name=collection_name)
                    
                    # æå–å‡½æ•°åç”¨äºå…ƒæ•°æ®æŸ¥è¯¢
                    chunk_names = [name.split('__')[-1] for name in changed_nodes]
                    where_clause = {"chunk_name": {"$in": chunk_names}}
                    
                    results = collection.get(where=where_clause, include=["embeddings"])
                    
                    if results and results['embeddings']:
                        # å¯¹æ‰€æœ‰æ‰¾åˆ°çš„å‘é‡å–å¹³å‡å€¼ï¼Œåˆ›å»ºä¸€ä¸ªç»¼åˆæŸ¥è¯¢å‘é‡
                        embeddings = np.array(results['embeddings'])
                        query_embedding = np.mean(embeddings, axis=0).tolist()
                        print(f"Successfully retrieved and averaged {len(embeddings)} vectors.")
                    else:
                        print("Warning: 'fast' mode: No pre-computed vectors found for changed functions. Falling back to 'diff'.")
                        query_text = file_changes_analysis['diff_query_text']
                except Exception as e:
                    print(f"Error in 'fast' mode: {e}. Falling back to 'diff'.")
                    query_text = file_changes_analysis['diff_query_text']
            else:
                print("Warning: 'fast' mode selected but no changed functions identified. Falling back to 'diff'.")
                query_text = file_changes_analysis['diff_query_text']
        
        else: # é»˜è®¤ 'diff' æ¨¡å¼
            print("Strategy: Using PR diff content.")
            query_text = file_changes_analysis['diff_query_text']
        
        # ================== A/B/C æµ‹è¯•é€»è¾‘åˆ†æ”¯ END ==================

        if not query_text and not query_embedding:
            print("No query text or embedding for RAG. Skipping.")
            return [], []

        # --- retriever ç°åœ¨éœ€è¦èƒ½åŒæ—¶å¤„ç† text æˆ– embedding ---
        # (è¿™éœ€è¦å¯¹ retriever.py åšä¸€ä¸ªå°çš„ä¿®æ”¹)
        print(f"Retrieving Top-K={top_k} relevant code snippets...")
        if query_embedding:
             relevant_chunks_docs, relevant_chunks_metas = retriever.retrieve_relevant_code(None, repo.name, n_results=top_k, query_embedding=query_embedding)
        else:
             relevant_chunks_docs, relevant_chunks_metas = retriever.retrieve_relevant_code(query_text, repo.name, n_results=top_k)

        if not relevant_chunks_docs:
            print("No relevant code snippets found in the knowledge base.")
            return [], []

        # --- ä¾èµ–é“¾åˆ†æä¸ Prompt ç»„è£…é€»è¾‘ (å’ŒåŸæ¥ä¿æŒä¸€è‡´) ---
        # (çœç•¥äº†è¿™éƒ¨åˆ†ä»£ç ï¼Œå› ä¸ºå®ƒå’Œä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒï¼Œä»¥ä¿æŒç®€æ´)
        context_list = []
        dependency_chains = []
        changed_function_nodes = file_changes_analysis["changed_function_nodes"]
        if call_graph and changed_function_nodes:
            for i, meta in enumerate(relevant_chunks_metas):
                candidate_module_path = lang_registry.strip_extension(meta['file_path']).replace('/', '.').replace('\\', '.')
                candidate_name = meta['chunk_name']
                candidate_full_name = f"{candidate_module_path.replace('.', '__')}__{candidate_name}"

                aligned_candidate_name = self._align_node_name(candidate_full_name, call_graph)
                if not aligned_candidate_name:
                    continue

                for changed_func_name in changed_function_nodes:
                    aligned_changed_name = self._align_node_name(changed_func_name, call_graph)
                    if not aligned_changed_name:
                        continue

                    # æ­£å‘æœç´¢
                    path = graph_parser.find_path(call_graph, start_node=aligned_changed_name, end_node=aligned_candidate_name)
                    if path:
                        dependency_chains.append(" -> ".join(path))

                    # åå‘æœç´¢
                    path_reversed = graph_parser.find_path(call_graph, start_node=aligned_candidate_name, end_node=aligned_changed_name)
                    if path_reversed:
                        dependency_chains.append(" -> ".join(path_reversed))
        
        # æ‹¼æ¥ Prompt
        if dependency_chains:
            context_list.append("\n---\n")
            context_list.append("### â›“ï¸ CRITICAL: Dependency Chains Detected!\n")
            context_list.append("These functions are connected to your changes through direct or indirect calls. Changes in one might unexpectedly affect the other.\n")
            for chain in sorted(list(set(dependency_chains))):
                context_list.append(f"- `{chain}`\n")
            context_list.append("\n---\n")
        
        context_list.append("\n---\n")
        context_list.append("### ğŸ“š Potentially Related Code from the Repository\n")
        context_list.append("Below are the most semantically related code snippets based on content similarity:\n\n")
        for i, snippet in enumerate(relevant_chunks_docs):
            fence_tag = ""
            if i < len(relevant_chunks_metas):
                fence_tag = lang_registry.get_code_fence_tag(relevant_chunks_metas[i].get('file_path', ''))
            context_list.append(f"```{fence_tag}\n")
            context_list.append(snippet)
            context_list.append("\n```\n\n")
            
        return relevant_chunks_docs, dependency_chains

    # ... _align_node_name å’Œ post_comment æ–¹æ³•ä¿æŒä¸å˜ ...
    def _align_node_name(self, node_name: str, graph: dict) -> Optional[str]:
        """ã€å·¥å…·å‡½æ•°ã€‘åŠ¨æ€å¯¹é½èŠ‚ç‚¹åï¼Œè§£å†³ pyan å‰ç¼€ä¸ä¸€è‡´é—®é¢˜ã€‚"""
        if node_name in graph:
            return node_name
        
        # å°è¯•ç§»é™¤è·¯å¾„å‰ç¼€ï¼Œç›´åˆ°åŒ¹é…æˆ–æ— æ³•å†åˆ†
        parts = node_name.split('__')
        for i in range(1, len(parts)):
            potential_match = "__".join(parts[i:])
            if potential_match in graph:
                return potential_match
        return None

    def post_comment(self, repo_name: str, pr_number: int, comment: str):
        try:
            repo = self.client.get_repo(repo_name)
            pr = repo.get_pull(pr_number)
            pr.create_issue_comment(comment)
            print(f"Successfully posted a comment on PR #{pr_number}.")
        except GithubException as e:
            print(f"Error posting comment to GitHub: {e.status} {e.data}")
            raise
        except Exception as e:
            print(f"An unexpected error occurred in post_comment: {e}")
            raise